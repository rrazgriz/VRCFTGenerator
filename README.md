# VRCFTGenerator

> Generate VRChat systems for animating facial blendshapes that use Direct Blend Trees, with a fixed amount of layers to drive an unlimited amount of blendshapes with complex parameter configurations.
> Licensed as MIT.

Please see the VRCFT wiki for more information on [Blendshapes](https://github.com/benaclejames/VRCFaceTracking/wiki/Blend-Shapes-Setup) and [Parameter Setup](https://github.com/benaclejames/VRCFaceTracking/wiki/Parameters).

![M7HZqf71Ua](https://user-images.githubusercontent.com/47901762/182271879-e2adf52c-196f-4381-9848-98d6d03c82c7.png)

ðŸ›‘ **This tool is not intended to be plug-and-play**, I built it for my own reference and as a direct blendtree study, with help from Lyuma and Hai. I am releasing it as-is for others to reference, study, and maybe use. The code quality is not particularly good, and may contain bad practices, unused code, and be difficult to read.

âš  **Please take note**: this tool relies on having a version of Animator As Code that supports VRC parameter driver copy behavior. A version of the framework is included with this functionality added. When AAC is updated to support this, you can remove the "Framework" folder and use the main Animator As Code release from https://github.com/hai-vr/av3-animator-as-code. I will try to remember to remove this folder (and update the code, if necessary) once that change is merged.

### Known Issues

- If no float parameters are specified, the script will fail to run
- A binary cast layer will be generated even if no parameters are set to binary
- If "Raw" or "Averaged" parameters are selected for use, decode children slots will be generated for them with None motions. To avoid this, don't directly drive "Raw" or "Averaged" parameters

### Improvements To Make

- Automatically add/remove parameters from VRChat SDK Parameters Object
- Support Eye tracking parameters (cases are more complex)
- Casting layer currently transitions to self every frame, could be improved


## Usage

- Make a backup of your FX layer.
- Add the `VRCFTGenerator` folder to your unity project.
- Add the script `VRCFTGenerator.cs` to any object.
- Configure the script with:
  - Your Avatar's root gameobject
  - An asset container (generate a new Animation controller for this)
  - A Skinned Mesh Renderer for the blendshape animations to target
  - Configuration for Write Defaults and the desired smoothing parameter (`0.6` - `0.7` recommended)
  - The parameters you wish to control in the system, and their precision
- Click generate. The script should pop up a confirmation informing you of the parameter cost.
- Add the associated parameters to your Synced Parameters project. A `FaceTracking` bool is generated for easy toggling on and off; when off, all face tracking blendshapes and smoothed parameters will be driven to 0.

## Details

This tool uses direct blend trees to convert boolean-encoded decimals ([Binary Parameters](https://github.com/benaclejames/VRCFaceTracking/wiki/Parameters#binary-parameters)) to animator float parameters, drive and smooth animator float parameters, and drive blendshapes with the smoothed value. The generated system works as follows:

- A "BinaryCast" layer is created with a single state with a do-nothing animation. Using the VRC Parameter Driver state behavior's "Copy" behavior, all binary parameters are cast to a float parameter with their equivalent decimal value, and any "Negative" boolean flags are cast to floats for later use in blendtrees. This value is dependent on the selected precision of the parameter. This layer's state transitions to itself every frame, so the values are continuously updated.
- A "Decode" layer is created with a single direct blend tree, which does the following things:
  - Sum binary parameter float values into their final value
  - Decode **[Combined Parameters](https://github.com/benaclejames/VRCFaceTracking/wiki/Parameters#combined-lip-parameters)** into their constituent "raw" parameters (corresponding directly to SRAnipal Blendshapes)
  - Decode **Averaged Parameters** (parameters that drive two symmetric values to the same value, e.g. CheekPuffRight/CheekPuffLeft -> CheekPuff) into their constituent combined and raw parameters.
- A "Smoothing" layer is created with a direct blend tree and an "Off" state. The off state drives all blendshapes and smoothed parameters to 0. The direct blend tree does the following things:
  - Smooths the decoded parameters using discrete exponential smoothing
  - Drives blendshapes based on the smoothed values

There are a number of rules that have to be followed for these direct blend trees to work correctly with Write Defaults On or Off:

- Direct Blendtree Motions should not have an animation as a first-level child motion - another blend tree (1d, 2d, or direct) should be used instead.
- At the top level of the direct blendtree, a single normalizing parameter is used, with a value of `1/numberOfChildren` (for a 20-child blendtree, this would be `1/20 = 0.05`). This normalizes the blendtree's outputs correctly.
  - In blendtrees that are not the top level direct blendtrees (even direct ones), this value needs to be 1.0 instead.
  - Child motions can nest blendtrees as deep as is needed.
- Output animations (at the end of the blendtrees) need to have their keyframe values scaled by a factor of the number of children at the top level blendtree. For a 20-child blendtree, output animator parameters would need to be scaled to `20` instead of `1`, and blendshapes would need to be `2000` instead of `100`. Zero is unaffected.

When these rules are followed, direct blend trees appear to work as expected whether Write Defaults is on or off. 

## Extras

Thanks very much to:

- [Hai](https://github.com/hai-vr/) and [Lyuma](https://github.com/lyuma) for their immense expertise and help in figuring out a stable configuration for using direct blend trees.
- The [VRCFT](https://github.com/benaclejames/VRCFaceTracking) developers for their wonderful application.
- The [VRCFT Discord](https://discord.gg/Fh4FNehzKn) and for their encouragement to push this much further than I otherwise would've.
